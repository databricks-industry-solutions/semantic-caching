{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb225e14-b647-4655-8304-0f7e7d20a4d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This solution accelerator notebook is available at [Databricks Industry Solutions](https://github.com/databricks-industry-solutions/semantic-caching)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c4c5791-01cd-4c92-ab63-0b46968be7e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Create and deploy a standard RAG chain\n",
    "\n",
    "In this notebook, we will build a standard RAG chatbot without semantic caching to serve as a benchmark. We will utilize the [Databricks Mosaic AI Agent Framework](https://www.databricks.com/product/machine-learning/retrieval-augmented-generation), which enables rapid prototyping of the initial application. In the following cells, we will define a chain, log and register it using MLflow and Unity Catalog, and finally deploy it behind a [Databricks Mosaic AI Model Serving](https://docs.databricks.com/en/machine-learning/model-serving/index.html) endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee60d0db-56e8-405e-8dc3-6cb02b1a29ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cluster configuration\n",
    "We recommend using a cluster with the following specifications to run this solution accelerator:\n",
    "- Unity Catalog enabled cluster \n",
    "- Databricks Runtime 15.4 LTS ML or above\n",
    "- Single-node cluster: e.g. `m6id.2xlarge` on AWS or `Standard_D8ds_v4` on Azure Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eef3d071-52ce-4e56-b0c1-3fed75c65533",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install requirements"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bae83c7-f20e-4428-af10-d8096a634246",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load parameters"
    }
   },
   "outputs": [],
   "source": [
    "from config import Config\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5d43204-db2c-4b3c-a68e-e5a0f2ce4c50",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run init notebok"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./99_init $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "818f3922-1bb8-4171-bfab-f5bfbe9cf344",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Here, we define environment variables `HOST` and `TOKEN` for our Model Serving endpoint to authenticate against our Vector Search index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "757569e1-7e0c-4ce6-a6e6-c6f40fe88b72",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Define environmental variables"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HOST = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "os.environ['DATABRICKS_HOST'] = HOST\n",
    "os.environ['DATABRICKS_TOKEN'] = TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51bfa8de-ba1c-4822-bffb-ba7180c45b2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create and register a chain to MLflow \n",
    "\n",
    "The next cell defines a standard RAG chain using Langchain. When executed, it will write the content to the `chain/chain.py` file, which will then be used to log the chain in MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad1eb6c8-eebc-45cc-81e6-ef8741d520ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile chain/chain.py\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatDatabricks\n",
    "from langchain_community.vectorstores import DatabricksVectorSearch\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "from config import Config\n",
    "import mlflow\n",
    "import os\n",
    "\n",
    "# Enable MLflow Tracing\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "# load parameters\n",
    "config = Config()\n",
    "\n",
    "# Connect to the Vector Search Index\n",
    "vs_index = VectorSearchClient(\n",
    "    workspace_url=os.environ['DATABRICKS_HOST'],\n",
    "    personal_access_token=os.environ['DATABRICKS_TOKEN'],\n",
    "    disable_notice=True,\n",
    "    ).get_index(\n",
    "    endpoint_name=config.VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "    index_name=config.VS_INDEX_FULLNAME,\n",
    ")\n",
    "\n",
    "# Turn the Vector Search index into a LangChain retriever\n",
    "vector_search_as_retriever = DatabricksVectorSearch(\n",
    "    vs_index,\n",
    "    text_column=\"content\",\n",
    "    columns=[\"id\", \"content\", \"url\"],\n",
    ").as_retriever(search_kwargs={\"k\": 3}) # Number of search results that the retriever returns\n",
    "# Enable the RAG Studio Review App and MLFlow to properly display track and display retrieved chunks for evaluation\n",
    "mlflow.models.set_retriever_schema(primary_key=\"id\", text_column=\"content\", doc_uri=\"url\")\n",
    "\n",
    "# Method to format the docs returned by the retriever into the prompt (keep only the text from chunks)\n",
    "def format_context(docs):\n",
    "    chunk_contents = [f\"Passage: {d.page_content}\\n\" for d in docs]\n",
    "    return \"\".join(chunk_contents)\n",
    "\n",
    "# Prompt template to be used to prompt the LLM\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", f\"{config.LLM_PROMPT_TEMPLATE}\"),\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Our foundation model answering the final prompt\n",
    "model = ChatDatabricks(\n",
    "    endpoint=config.LLM_MODEL_SERVING_ENDPOINT_NAME,\n",
    "    extra_params={\"temperature\": 0.01, \"max_tokens\": 500}\n",
    ")\n",
    "\n",
    "# Return the string contents of the most recent messages: [{...}] from the user to be used as input question\n",
    "def extract_user_query_string(chat_messages_array):\n",
    "    return chat_messages_array[-1][\"content\"]\n",
    "\n",
    "# RAG Chain\n",
    "chain = (\n",
    "    {\n",
    "        \"question\": itemgetter(\"messages\") | RunnableLambda(extract_user_query_string),\n",
    "        \"context\": itemgetter(\"messages\")\n",
    "        | RunnableLambda(extract_user_query_string)\n",
    "        | vector_search_as_retriever\n",
    "        | RunnableLambda(format_context),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Tell MLflow logging where to find your chain.\n",
    "mlflow.models.set_model(model=chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffcebaba-3bfc-4946-9400-784fbdcf99d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this cell, we log the chain to MLflow. Note that we are passing `config.py` as a dependency, allowing the chain to load the necessary parameters when deployed to another compute environment or to a Model Serving endpoint. MLflow returns a trace of the inference that shows the detail breakdown of the latency and the input/output from each step in the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce7563bf-df0f-4982-adaa-377897ac158b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Log the model to MLflow\n",
    "config_file_path = \"config.py\"\n",
    "\n",
    "# Create a config file to be used by the chain\n",
    "with mlflow.start_run(run_name=f\"rag_chatbot\"):\n",
    "    logged_chain_info = mlflow.langchain.log_model(\n",
    "        lc_model=os.path.join(os.getcwd(), 'chain/chain.py'),  # Chain code file e.g., /path/to/the/chain.py \n",
    "        artifact_path=\"chain\",  # Required by MLflow\n",
    "        input_example=config.INPUT_EXAMPLE,  # MLflow will execute the chain before logging & capture it's output schema.\n",
    "        code_paths = [config_file_path], # Include the config file in the model\n",
    "    )\n",
    "\n",
    "# Test the chain locally\n",
    "chain = mlflow.langchain.load_model(logged_chain_info.model_uri)\n",
    "chain.invoke(config.INPUT_EXAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9e379ab-467e-4d06-ad17-4fbfa3fa3c8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If we are happy with the logged chain, we will go ahead and register the chain in Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23b45dd0-791c-44e4-96ab-1aaf9bb1ee76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register to UC\n",
    "uc_registered_model_info = mlflow.register_model(\n",
    "  model_uri=logged_chain_info.model_uri, \n",
    "  name=config.MODEL_FULLNAME\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d93f96d-6258-40d5-a6a1-b1bb1c164a83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy the chain to a Model Serving endpoint\n",
    "\n",
    "We deploy the chaing using custom functions defined in the `utils.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "687505ae-dcaf-4ebb-a192-7540ce0254aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "utils.deploy_model_serving_endpoint(\n",
    "  spark, \n",
    "  config.MODEL_FULLNAME,\n",
    "  config.CATALOG,\n",
    "  config.LOGGING_SCHEMA,\n",
    "  config.ENDPOINT_NAME,\n",
    "  HOST,\n",
    "  TOKEN,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "661a7c94-841f-4605-afcf-d0e56213ee75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Wait until the endpoint is ready. This may take some time (~15 minutes), so grab a coffee!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed20c9ad-fef1-4981-89bb-70a10818ab75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "utils.wait_for_model_serving_endpoint_to_be_ready(config.ENDPOINT_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a825876d-9522-4bbb-b185-608400b3271e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Once the endpoint is up and running, let's send a request and see how it responds. If the following cell fails with 404 Not Found error, take a minute and try re-running the cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9f726af-eed1-43b2-8ffa-663a4e26f29d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "data = {\n",
    "    \"inputs\": {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"content\": \"What is Model Serving?\",\n",
    "                \"role\": \"user\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "# Now, call the function with the correctly formatted data\n",
    "utils.send_request_to_endpoint(\n",
    "    config.ENDPOINT_NAME, \n",
    "    data,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13a17836-31b9-4924-8400-74df6761a956",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this notebook, we built a standard RAG chatbot without semantic caching to serve. We will use this chain to benchmark against the chain with semantic caching, which we will build in the next `03_rag_chatbot_with_cache` notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcdc85ef-8f2d-40b2-ba12-c84a497c163f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "© 2024 Databricks, Inc. All rights reserved. The source in this notebook is provided subject to the Databricks License."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02_rag_chatbot",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
