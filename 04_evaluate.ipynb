{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ba3c8db-a4d9-4358-9f9a-b1112c7065c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This solution accelerator notebook is available at [Databricks Industry Solutions](https://github.com/databricks-industry-solutions/semantic-caching)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20855d3d-3ac7-4da0-bb14-52b97685e80d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Evaluate the RAG chains with and without caching\n",
    "\n",
    "In the previous notebooks, we created and deployed RAG chains with and without semantic caching. Both are now up and running, ready to handle requests. In this notebook, we will conduct a benchmarking exercise to evaluate the latency reduction achieved by the cached chain and assess the trade-off in response quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3444932-af0f-400a-a850-e65f2fd7b055",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cluster configuration\n",
    "We recommend using a cluster with the following specifications to run this solution accelerator:\n",
    "- Unity Catalog enabled cluster \n",
    "- Databricks Runtime 15.4 LTS ML or above\n",
    "- Single-node cluster: e.g. `m6id.2xlarge` on AWS or `Standard_D8ds_v4` on Azure Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f600c97-4b70-4ce0-9da2-67515e9cf867",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load parameters"
    }
   },
   "outputs": [],
   "source": [
    "from config import Config\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc7e77e0-57c2-4f43-8fe8-b6d220141cf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data preparation\n",
    "\n",
    "For the benchmarking exercise, we will use a hundred synthesized questions stored in `data/synthetic_questions_100.csv`. To create these, we first generated ten questions related to Databricks Machine Learning product features using [dbrx-instruct](https://e2-demo-field-eng.cloud.databricks.com/editor/notebooks/1284968239746639?o=1444828305810485#command/1284968239757668). We then expanded these by reformulating each of the ten questions slightly, without changing their meaning, generating ten variations of each. This resulted in a hundred questions in total. For this process, we used [Meta Llama 3.1 70B Instruct](https://docs.databricks.com/en/machine-learning/foundation-models/supported-models.html#meta-llama-31-70b-instruct).\n",
    "\n",
    "We read this dataset in and save it into a delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c652ab98-82e0-4d6b-aa63-26e8743eeaea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/synthetic_questions_100.csv') # this is a small sample of 100 questions\n",
    "df = spark.createDataFrame(df) # convert to a Spark DataFrame\n",
    "df.write.mode('overwrite').saveAsTable(f'{config.CATALOG}.{config.SCHEMA}.synthetic_questions_100') # save to a table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3e33b88-65e5-483d-8432-40f5089f3efb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Next, we will format the questions so that we can apply the chain directly later. We store the formatted dataset in another delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56881b5c-195f-426a-ab2f-53206ac7d2af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {config.CATALOG}.{config.SCHEMA}.synthetic_questions_100_formatted AS\n",
    "SELECT STRUCT(ARRAY(STRUCT(question AS content, \"user\" AS role)) AS messages) AS question, base as base\n",
    "FROM {config.CATALOG}.{config.SCHEMA}.synthetic_questions_100;\n",
    "\"\"\")\n",
    "\n",
    "df = spark.table(f'{config.CATALOG}.{config.SCHEMA}.synthetic_questions_100_formatted')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df8fa3a0-7df3-4fc5-932b-642d35229918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test standard rag chain endpoint\n",
    "\n",
    "Now that we have our test dataset, we are going to go ahead and test the standard RAG chain endpoint. We will use [ai_query](https://docs.databricks.com/en/sql/language-manual/functions/ai_query.html) to apply the chain to the formatted table. We write the result out to another delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78f35422-964d-406e-8a63-547f8fa75571",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load testing standard RAG chain"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {config.CATALOG}.{config.SCHEMA}.standard_rag_chain_results AS\n",
    "SELECT question, ai_query(\n",
    "  'standard_rag_chatbot',\n",
    "  question,\n",
    "  returnType => 'STRUCT<choices:ARRAY<STRING>>'\n",
    "  ) AS prediction, base\n",
    "FROM {config.CATALOG}.{config.SCHEMA}.synthetic_questions_100_formatted;\n",
    "\"\"\")\n",
    "\n",
    "standard_rag_chain_results = spark.table(f'{config.CATALOG}.{config.SCHEMA}.standard_rag_chain_results')\n",
    "display(standard_rag_chain_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c9d3d2-347d-402a-9896-cbc01be9fa5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Test rag chain with cache endpoint\n",
    "\n",
    "We are now going to test the RAG chain with cache endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7e1d06d-e56d-4b9b-8e8e-b28512e11a91",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load testing RAG chain with cache"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {config.CATALOG}.{config.SCHEMA}.rag_chain_with_cache_results AS\n",
    "SELECT question, ai_query(\n",
    "    'rag_chatbot_with_cache',\n",
    "    question,\n",
    "    returnType => 'STRUCT<choices:ARRAY<STRING>>'\n",
    "  ) AS prediction, base\n",
    "FROM {config.CATALOG}.{config.SCHEMA}.synthetic_questions_100_formatted;\n",
    "\"\"\")\n",
    "\n",
    "rag_chain_with_cache_results = spark.table(f'{config.CATALOG}.{config.SCHEMA}.rag_chain_with_cache_results')\n",
    "display(rag_chain_with_cache_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d596b03b-58a8-4a9e-a99a-bf57d3f23ce2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Just by looking at the execution time, we notice that the chain with cache ran more than 2x faster than the chain without."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "766a9446-7125-4058-927a-69246058a66f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Evaluate results using MLflow\n",
    "\n",
    "We will begin by evaluating the quality of the responses from both endpoints. Since the 100 questions were derived from the original 10 through reformulation (without changing their meaning), we can use the answers to the original questions as the ground truth for evaluating the responses to the 100 variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e38a262-d8e3-4545-8f97-af9fd398bcfb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reading in the original 10 questions and answers"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "synthetic_qa = []\n",
    "with open('data/synthetic_qa.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        synthetic_qa.append(json.loads(line))\n",
    "\n",
    "display(synthetic_qa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cae7d8f6-f10d-4945-a707-c1c4d7489e06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We construct an evaluation dataset for the standard RAG chain and the chain with the cache. The `prediction` column stores the responses from the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d58098a-667a-4720-a039-f3f440b39d56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "evaluation_standard = spark.table(f'{config.CATALOG}.{config.SCHEMA}.standard_rag_chain_results').toPandas()\n",
    "evaluation_cache = spark.table(f'{config.CATALOG}.{config.SCHEMA}.rag_chain_with_cache_results').toPandas()\n",
    "\n",
    "evaluation_standard[\"question\"] = evaluation_standard[\"question\"].apply(lambda x: x[\"messages\"][0][\"content\"])\n",
    "evaluation_standard[\"prediction\"] = evaluation_standard[\"prediction\"].apply(lambda x: json.loads(x[\"choices\"][0])[\"message\"][\"content\"])\n",
    "\n",
    "evaluation_cache[\"question\"] = evaluation_cache[\"question\"].apply(lambda x: x[\"messages\"][0][\"content\"])\n",
    "evaluation_cache[\"prediction\"] = evaluation_cache[\"prediction\"].apply(lambda x: json.loads(x[\"choices\"][0])[\"message\"][\"content\"])\n",
    "\n",
    "labels = pd.DataFrame(synthetic_qa).drop([\"question\"], axis=1)\n",
    "\n",
    "evaluation_standard = evaluation_standard.merge(labels, on='base')\n",
    "evaluation_cache = evaluation_cache.merge(labels, on='base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ea44695-5ca9-427c-bb9d-48b41eb93e89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "evaluation_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb3e56ce-79cd-4f2e-a97f-a051c1d1464e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "evaluation_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b68486a-f11b-4c27-bf4d-0cf37cea83b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "To assess the quality of the responses, we will use [`mlflow.evaluate`](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.evaluate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e0fdb47-94dc-4b9c-a0db-3164a93d0677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.deployments import set_deployments_target\n",
    "\n",
    "set_deployments_target(\"databricks\")\n",
    "judge_model = \"endpoints:/databricks-meta-llama-3-1-70b-instruct\" # this is the model endpont you want to use as a judge\n",
    "\n",
    "# Run evaluation for the standard chain\n",
    "with mlflow.start_run(run_name=\"evaluation_standard\"):\n",
    "    standard_results = mlflow.evaluate(        \n",
    "        data=evaluation_standard,\n",
    "        targets=\"answer\",\n",
    "        predictions=\"prediction\",\n",
    "        model_type=\"question-answering\",\n",
    "        extra_metrics=[\n",
    "          mlflow.metrics.genai.answer_similarity(model=judge_model), \n",
    "          mlflow.metrics.genai.answer_correctness(model=judge_model),\n",
    "          mlflow.metrics.genai.answer_relevance(model=judge_model),\n",
    "          ],\n",
    "        evaluator_config={\n",
    "            'col_mapping': {'inputs': 'question'}\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Run evaluation for the chain with cache\n",
    "with mlflow.start_run(run_name=\"evaluation_cache\"):\n",
    "    cache_results = mlflow.evaluate(        \n",
    "        data=evaluation_cache,\n",
    "        targets=\"answer\",\n",
    "        predictions=\"prediction\",\n",
    "        model_type=\"question-answering\",\n",
    "        extra_metrics=[\n",
    "          mlflow.metrics.genai.answer_similarity(model=judge_model), \n",
    "          mlflow.metrics.genai.answer_correctness(model=judge_model),\n",
    "          mlflow.metrics.genai.answer_relevance(model=judge_model),\n",
    "          ],\n",
    "        evaluator_config={\n",
    "            'col_mapping': {'inputs': 'question'}\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a932996-c09f-400d-8ab9-352afc58b319",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's print out the aggregated statistics of the quality metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ea3382d-0cc5-4832-ba09-71f05628e0fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"See aggregated evaluation results below: \\n{standard_results.metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cc059f7-dd8b-4294-a884-693b65b148d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"See aggregated evaluation results below: \\n{cache_results.metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59d99255-1b53-4aa8-b8da-f5a21e40aadc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The evaluation results show that the standard RAG chain performed slightly better on the metrics `answer_relevance/v1/mean` (scoring `4.63` vs. `4.53`). This minor drop in quality is expected when responses are retrieved from the cache. The key takeaway is to assess whether the difference is acceptable given the cost and latency reductions provided by the caching solution. Ultimately, the decision should be based on how these trade-offs impact the business value of your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07a42566-b32e-46d7-9e48-bb0a10905926",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Query the Inference tables\n",
    "\n",
    "Each request and response that hits the endpoint can be logged to an [inference table](https://docs.databricks.com/en/machine-learning/model-serving/inference-tables.html) along with its [trace](https://docs.databricks.com/en/mlflow/mlflow-tracing.html#use-mlflow-tracing-in-production). These tables are particularly useful for debugging and auditing. We will query the inference tables for both endpoints to gain insights into performance optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfced0a3-9b1c-4014-9718-882e7c15e9f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# You can just query the inference table \n",
    "standard_log = spark.read.table(f\"{config.CATALOG}.{config.LOGGING_SCHEMA}.standard_rag_chatbot_payload\").toPandas()\n",
    "display(standard_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ac1f87a-72fc-415b-9c17-bb26617650ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cache_log = spark.read.table(f\"{config.CATALOG_CACHE}.{config.LOGGING_SCHEMA_CACHE}.rag_chatbot_with_cache_payload\").toPandas()\n",
    "display(cache_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75e61118-c305-4f4f-ac7c-0072ec19b8e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's calculate the mean execution time per query. We see a significant drop in the chain with cache, which is directly translatable to cost reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96eefc80-9ec7-4dc6-9d66-ef14fd691054",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"standard rag chain mean execution time: {round(standard_log['execution_time_ms'].mean()/1000, 4)} seconds\")\n",
    "print(f\"rag chain with cache mean execution time: {round(cache_log['execution_time_ms'].mean()/1000, 4)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88bbf921-df41-41f4-a5fb-cf3e666e06c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "One of the important KPIs for a caching solution is the hit rate. We can retrieve this information from the traces stored in the inference table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "159285d1-237b-494d-90a0-0771f0150c53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "cache_trace = np.array(\n",
    "    cache_log[\"response\"].apply(lambda x: 1 if len(json.loads(x)[\"databricks_output\"][\"trace\"][\"data\"][\"spans\"]) == 6 else 0)\n",
    ")\n",
    "print(f\"Number of times the query hit the cache: {cache_trace.sum()}/100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbb1e444-851b-4e5d-9359-d0fd348a9074",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cache_execution_time = np.array(\n",
    "    cache_log[\"response\"].apply(lambda x: json.loads(x)[\"databricks_output\"][\"trace\"][\"info\"][\"execution_time_ms\"] if len(json.loads(x)[\"databricks_output\"][\"trace\"][\"data\"][\"spans\"]) == 6 else 0)\n",
    ")\n",
    "print(f\"The mean execution time of the queries that hit the cache: {round(cache_execution_time.sum()/cache_trace.sum()/1000, 4)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "827eb042-d4a4-421e-bcca-a2c27f6bc633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this notebook, we conducted a benchmarking exercise to compare the solutions with and without semantic caching. For this specific dataset, we observed a significant reduction in both cost and latency, though with a slight trade-off in quality. It’s important to emphasize that every use case should carefully assess the impact of these gains and losses on business objectives before making a final decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16e0125c-76db-487e-aa99-77571e29dac6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "© 2024 Databricks, Inc. All rights reserved. The source in this notebook is provided subject to the Databricks License."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "04_evaluate",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
