{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "858e2690-8d96-4d98-a888-a5a95b102a2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This solution accelerator notebook is available at [Databricks Industry Solutions](https://github.com/databricks-industry-solutions/semantic-caching)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a035928-c8d3-44e4-b765-92daacd1f01a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Create and deploy a RAG chain with semantic caching\n",
    "\n",
    "In this notebook, we will build a RAG chatbot with semantic caching. To do this, we first need to create and warm up our cache. Weâ€™ll use [Mosaic AI Vector Search](https://docs.databricks.com/en/generative-ai/vector-search.html) for semantic caching, taking advantage of its high-performance similarity search. In the following cells, we will create and warm the cache, build a chain with a semantic caching layer, log and register it using MLflow and Unity Catalog, and finally deploy it behind a [Databricks Mosaic AI Model Serving](https://docs.databricks.com/en/machine-learning/model-serving/index.html) endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "190d25f8-b302-4cae-a0c0-afc50b687178",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cluster configuration\n",
    "We recommend using a cluster with the following specifications to run this solution accelerator:\n",
    "- Unity Catalog enabled cluster \n",
    "- Databricks Runtime 15.4 LTS ML or above\n",
    "- Single-node cluster: e.g. `m6id.2xlarge` on AWS or `Standard_D8ds_v4` on Azure Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3e9d172-e238-496e-a6c8-b1ed01c9c66f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install requirements"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt --quiet\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bb0bac9-5e24-4c95-8602-1dcc8ba04a81",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load parameters"
    }
   },
   "outputs": [],
   "source": [
    "from config import Config\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfaad0e5-e598-4b68-9249-a7ea7b996c92",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Run init notebok"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%run ./99_init $reset_all_data=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "394feac7-bb8c-4267-9e97-f187f2fc9678",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Here, we define environmental variables `HOST` and `TOKEN` for our Model Serving endpoint to authenticate against our Vector Search index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20e9fd40-df1b-4c93-a8e3-8550e54360f7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Set environmental variables"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HOST = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().get()\n",
    "TOKEN = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().get()\n",
    "\n",
    "os.environ['DATABRICKS_HOST'] = HOST\n",
    "os.environ['DATABRICKS_TOKEN'] = TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9fcea94-82d3-484c-b4ad-bc6b94451c9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create and warm a cache "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecde0155-bba9-4439-b516-58121e2531a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We instantiate a Vector Search client to interact with a Vector Search endpoint to create a cache. This will be an additional Vector Search Index, which, if the cache is hit, can immediately route the question to the answer in the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38882af4-7864-4d3c-ad3b-afa1c2f4ae5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from cache import Cache\n",
    "\n",
    "# Create a Vector Search Client\n",
    "vsc = VectorSearchClient(\n",
    "    workspace_url=HOST,\n",
    "    personal_access_token=TOKEN,\n",
    "    disable_notice=True,\n",
    "    )\n",
    "\n",
    "# Initialize the cache\n",
    "semantic_cache = Cache(vsc, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bfecc3b-39fb-4296-8a35-e2b63ba45134",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We first delete the cache if it already exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3932d843-f31a-49d6-9119-2782e5b78f51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "semantic_cache.clear_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54a4addc-29cf-4df0-be04-ba518399eefa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We then create a cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ab8f9a5-c024-46b3-b495-25fab68e55e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "semantic_cache.create_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a80596e-09b1-4c3e-8d49-22b3d5e87097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We finally load the cache with predefined Q&A pairs: i.e., `/data/synthetic_qa.txt`. This synthetic dataset contains a set of questions that have already been answered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db46e68b-09b1-4b23-a1dc-caa42f9bfe6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "semantic_cache.warm_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1484d3c1-d694-45ec-b898-eb874bfbdd34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Create and register a chain to MLflow \n",
    "\n",
    "The next cell defines our RAG chain with semantic cache using Langchain. When executed, it will write the content to the `chain/chain_cache.py` file, which will then be used to log the chain in MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce5a53eb-82bf-4c95-b58a-4801b1cb1c1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%%writefile chain/chain_cache.py\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from langchain_community.vectorstores import DatabricksVectorSearch\n",
    "from langchain.schema.runnable import RunnableLambda, RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_models import ChatDatabricks\n",
    "from operator import itemgetter\n",
    "from datetime import datetime\n",
    "from uuid import uuid4\n",
    "import os\n",
    "import mlflow\n",
    "from cache import Cache\n",
    "from config import Config\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.getLogger(\"py4j.java_gateway\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"py4j.clientserver\").setLevel(logging.ERROR)\n",
    "\n",
    "# Enable MLflow Tracing\n",
    "mlflow.langchain.autolog()\n",
    "\n",
    "# Get configuration\n",
    "config = Config()\n",
    "\n",
    "# Connect to Vector Search\n",
    "vsc = VectorSearchClient(\n",
    "    workspace_url=os.environ['DATABRICKS_HOST'],\n",
    "    personal_access_token=os.environ['DATABRICKS_TOKEN'],\n",
    "    disable_notice=True,\n",
    ")\n",
    "\n",
    "# Get the Vector Search index\n",
    "vs_index = vsc.get_index(\n",
    "    index_name=config.VS_INDEX_FULLNAME,\n",
    "    endpoint_name=config.VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "    )\n",
    "\n",
    "# Instantiate a Cache object\n",
    "semantic_cache = Cache(vsc, config)\n",
    "\n",
    "# Turn the Vector Search index into a LangChain retriever\n",
    "vector_search_as_retriever = DatabricksVectorSearch(\n",
    "    vs_index,\n",
    "    text_column=\"content\",\n",
    "    columns=[\"id\", \"content\", \"url\"],\n",
    ").as_retriever(search_kwargs={\"k\": 3}) # Number of search results that the retriever returns\n",
    "\n",
    "# Method to retrieve the context from the Vector Search index\n",
    "def retrieve_context(qa):\n",
    "    return vector_search_as_retriever.invoke(qa[\"question\"])\n",
    "\n",
    "# Enable the RAG Studio Review App and MLFlow to properly display track and display retrieved chunks for evaluation\n",
    "mlflow.models.set_retriever_schema(primary_key=\"id\", text_column=\"content\", doc_uri=\"url\")\n",
    "\n",
    "# Method to format the docs returned by the retriever into the prompt (keep only the text from chunks)\n",
    "def format_context(docs):\n",
    "    chunk_contents = [f\"Passage: {d.page_content}\\n\" for d in docs]\n",
    "    return \"\".join(chunk_contents)\n",
    "\n",
    "# Create a prompt template for response generation\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", f\"{config.LLM_PROMPT_TEMPLATE}\"),\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define our foundation model answering the final prompt\n",
    "model = ChatDatabricks(\n",
    "    endpoint=config.LLM_MODEL_SERVING_ENDPOINT_NAME,\n",
    "    extra_params={\"temperature\": 0.01, \"max_tokens\": 500}\n",
    ")\n",
    "\n",
    "# Call the foundation model\n",
    "def call_model(prompt):\n",
    "    response = model.invoke(prompt)\n",
    "    semantic_cache.store_in_cache(\n",
    "        question = prompt.dict()['messages'][1]['content'], \n",
    "        answer = response.content\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# Return the string contents of the most recent messages: [{...}] from the user to be used as input question\n",
    "def extract_user_query_string(chat_messages_array):\n",
    "    return chat_messages_array[-1][\"content\"]\n",
    "\n",
    "# Router to determine which subsequent step to be executed\n",
    "def router(qa):\n",
    "    if qa[\"answer\"] == \"\":\n",
    "        return rag_chain\n",
    "    else:\n",
    "        return (qa[\"answer\"])\n",
    "\n",
    "# RAG chain\n",
    "rag_chain = (\n",
    "    {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"context\": RunnablePassthrough()\n",
    "        | RunnableLambda(retrieve_context)\n",
    "        | RunnableLambda(format_context),\n",
    "    }\n",
    "    | prompt\n",
    "    | RunnableLambda(call_model)\n",
    ")\n",
    "\n",
    "# Full chain with cache\n",
    "full_chain = (\n",
    "    itemgetter(\"messages\")\n",
    "    | RunnableLambda(extract_user_query_string)\n",
    "    | RunnableLambda(semantic_cache.get_from_cache)\n",
    "    | RunnableLambda(router)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Tell MLflow logging where to find your chain.\n",
    "mlflow.models.set_model(model=full_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fee41cb3-cf0e-4171-ba25-d100d3a0e4bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this cell, we log the chain to MLflow. Note that this time we are passing `cache.py` and `utils.py` along with `config.py` as dependencies, allowing the chain to also load custom classes and functions needed to another compute environment or to a Model Serving endpoint. MLflow returns a trace of the inference that shows the detail breakdown of the latency and the input/output from each step in the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fd9526c-9bc1-4dc7-bfd5-c70742e8dbb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Log the model to MLflow\n",
    "config_file_path = \"config.py\"\n",
    "cache_file_path = \"cache.py\"\n",
    "utils_file_path = \"utils.py\"\n",
    "\n",
    "with mlflow.start_run(run_name=f\"rag_chatbot\"):\n",
    "    logged_chain_info = mlflow.langchain.log_model(\n",
    "        lc_model=os.path.join(os.getcwd(), 'chain/chain_cache.py'),  # Chain code file e.g., /path/to/the/chain.py \n",
    "        artifact_path=\"chain\",  # Required by MLflow\n",
    "        input_example=config.INPUT_EXAMPLE,  # MLflow will execute the chain before logging & capture it's output schema.\n",
    "        code_paths = [cache_file_path, config_file_path, utils_file_path],\n",
    "    )\n",
    "\n",
    "# Test the chain locally\n",
    "chain = mlflow.langchain.load_model(logged_chain_info.model_uri)\n",
    "chain.invoke(config.INPUT_EXAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc468b0f-8aba-40dd-a77a-2d692f4ce04f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's ask a question to the chain that we know a similar question has not been asked before therefore doesn't exist in the cache. We see in the trace that the entire chain is indeed executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d50a04d9-cc86-45c5-bc59-b8088b516567",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chain.invoke({'messages': [{'content': \"How does Databricks' feature Genie automate feature engineering for machine learning models?\", 'role': 'user'}]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7933dc56-4bb8-422b-9067-fabd0faeb5ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If we reformulate the question without changing the meaning, we get the response from the cache, as the question has been upserted into the cache. We see this in the trace and the execution time is less than half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa922c6f-0c78-4b52-a9a9-730f00b7583e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chain.invoke({'messages': [{'content': \"What is the role of Databricks' feature Genie in automating feature engineering for machine learning models?\", 'role': 'user'}]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efa61283-a3ab-4aed-9ae4-bada466ad779",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Where to set the similarity threshold -`0.01` in this demo defined in `config.py`- is arguably the most important design decision you need to make for your solution. A threshold that is too high will reduce the hit rate and undermine the effect of semantic caching, but a threshold too low could generate many false positives. There is a fine balance you would need to strike. To make an informed decision, refer to the exploratory data analysis performed in the `00_introduction` notebook.\n",
    "\n",
    "If we are happy with the chain, we will go ahead and register the chain in Unity Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88c0db72-6f73-4bbc-ba9b-dc0d3b8281f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register to UC\n",
    "uc_registered_model_info = mlflow.register_model(model_uri=logged_chain_info.model_uri, name=config.MODEL_FULLNAME_CACHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "420a9a7f-a7f3-440d-8119-5798fcc0cbd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Deploy the chain to a Model Serving endpoint\n",
    "\n",
    "We deploy the chain using custom functions defined in the `utils.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec1b7c5f-2056-48c4-a753-fd445991c9be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "utils.deploy_model_serving_endpoint(\n",
    "  spark, \n",
    "  config.MODEL_FULLNAME_CACHE,\n",
    "  config.CATALOG_CACHE,\n",
    "  config.LOGGING_SCHEMA_CACHE,\n",
    "  config.ENDPOINT_NAME_CACHE,\n",
    "  HOST,\n",
    "  TOKEN,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17db5538-2c8b-4b1f-8e6d-318afc04d2a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Wait until the endpoint is ready. This may take some time (~15 minutes), so grab a coffee!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "295e33fd-53b4-4aa8-b4ca-16ce59228d58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "utils.wait_for_model_serving_endpoint_to_be_ready(config.ENDPOINT_NAME_CACHE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "269e05d4-d7fa-4fc0-8900-f54165b9ac05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Once the endpoint is up and running, let's send a request and see how it responds. If the following cell fails with 404 Not Found error, take a minute and try re-running the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec532106-fb4b-497a-9a28-0756f173dd34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "data = {\n",
    "    \"inputs\": {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"content\": \"What is Model Serving?\",\n",
    "                \"role\": \"user\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "# Now, call the function with the correctly formatted data\n",
    "utils.send_request_to_endpoint(config.ENDPOINT_NAME_CACHE, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c6198dd-0760-4b51-be78-5f097d650043",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In this notebook, we built a RAG chatbot with semantic caching. In the next `04_evaluate` notebook, we will compare the two chains we built. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f287a822-1029-4a02-9c83-cb40856ebe9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Â© 2024 Databricks, Inc. All rights reserved. The source in this notebook is provided subject to the Databricks License."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "03_rag_chatbot_with_cache",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
